{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used four datasets for our analysis and prediction. Each dataset is prepared individually, and joined on a common index of lat, lon, year for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import descartes\n",
    "from itertools import chain\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chart_studio.plotly as py \n",
    "from chart_studio.plotly import plot, iplot\n",
    "\n",
    "import plotly.graph_objs as go \n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMMON DATA PROCESSING FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first define a common loading procedure that works for most of the datasets considered here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in the json file with lat lon and state county mapping for lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Lat/Lon data\n",
    "with open('data/latlon.json', 'r') as fp:\n",
    "    lat_lon_json = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. FUNCTION TO LOAD AND MERGE RAW FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load(subpath):\n",
    "    path = os.path.join('data/raw/', subpath, '*.csv')\n",
    "    all_files = glob.glob(path)\n",
    "\n",
    "    li = []\n",
    "    for filename in all_files:\n",
    "        df = pd.read_csv(filename, index_col=None)\n",
    "        li.append(df)\n",
    "\n",
    "    return pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. FUNCTION TO GET LAT LON PAIRS FROM JSON FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_lat_lon(q):\n",
    "    if q in lat_lon_json:\n",
    "        return lat_lon_json[q]\n",
    "    else:\n",
    "        return [None, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. FUNCTION TO CHANGE PRECISION OF LAT LON PAIRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _change_precision(a, prec=0):\n",
    "    acc = []\n",
    "    for e in a:\n",
    "        if e is None:\n",
    "            acc.append(None)\n",
    "        else:\n",
    "            acc.append(round(float(e), prec))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA READING AND CLEANING FUNCTIONS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LOAD TRI DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the TRI dataset into a common format, we will be performing following transformations:\n",
    "\n",
    "1. Clean the column names to remove spaces and capitals\n",
    "2. Select relevant columns \n",
    "3. Pivot on industry since it is categorical\n",
    "4. Fix the schema for the columns which will become the index (lat, lon, year)\n",
    "5. Change precision of lat and lon\n",
    "6. Regroup by year, lat, lon and aggregate by sum\n",
    "\n",
    "It's good that the TRI dataset comes with lat lon pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file\n",
    "combined = _load('tri')\n",
    "\n",
    "# Clean column names\n",
    "cols = combined.columns\n",
    "cols = [c.split(' - ')[-1] for c in cols]\n",
    "cols = [c.split('. ')[-1].lower() for c in cols]\n",
    "combined.columns = cols\n",
    "combined = combined.rename(columns={'production wste (8.1-8.7)': 'production waste',\n",
    "                                        'latitude': 'lat', 'longitude': 'lon'})\n",
    "\n",
    "# Select Relevant\n",
    "sel = ['year', 'lat', 'lon', 'industry sector',\n",
    "           'fugitive air', 'stack air', 'water', 'underground',\n",
    "           'underground cl i', 'underground c ii-v', 'landfills',\n",
    "           'rcra c landfill', 'other landfills', 'land treatment',\n",
    "           'surface impndmnt', 'rcra surface im', 'other surface i',\n",
    "           'on-site release total', 'trns rlse', 'trns trt',\n",
    "           'total transfers', 'm10', 'm41', 'm62', 'm40 metal', 'm61 metal', 'm71',\n",
    "           'm81', 'm82', 'm72', 'm63', 'm66', 'm67', 'm64', 'm65', 'm73', 'm79',\n",
    "           'm90', 'm94', 'm99', 'off-site release total', 'm20', 'm24', 'm26',\n",
    "           'm28', 'm93', 'off-site recycled total', 'm56', 'm92',\n",
    "           'm40 non-metal', 'm50', 'm54',\n",
    "           'm61 non-metal', 'm69', 'm95', 'off-site treated total',\n",
    "           'total transfer', 'total releases', 'releases', 'on-site contained',\n",
    "           'off-site contain', 'production waste']\n",
    "\n",
    "combined = combined[sel]\n",
    "combined.columns = [c.replace(' ', '_') for c in combined.columns]\n",
    "\n",
    "# Pivot industry\n",
    "d = pd.get_dummies(combined['industry_sector'], prefix='sector')\n",
    "df = pd.concat([combined, d], axis=1).drop(['industry_sector'], axis=1)\n",
    "\n",
    "# Fix schema\n",
    "df = df.dropna()\n",
    "df.year = df.year.astype(int)\n",
    "df.lat = df.lat.astype(int)\n",
    "df.lon = df.lon.astype(int)\n",
    "\n",
    "# Shift Precision\n",
    "df.lat = _change_precision(df.lat)\n",
    "df.lon = _change_precision(df.lon)\n",
    "\n",
    "# Group up\n",
    "df = df.dropna().groupby(by=['year', 'lat', 'lon'], as_index=False).sum()\n",
    "\n",
    "tri = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LOAD AQI DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the AQI dataset into a common format, we will be performing following transformations:\n",
    "\n",
    "1. Clean the column names to remove spaces and capitals\n",
    "2. Select relevant columns (as defined in function)\n",
    "3. Convert date to year by remove the month and day\n",
    "4. Pivot on defining_parameter since it is categorical.\n",
    "5. Regroup by 'year', 'state_name', 'county_name', aggregating by mean.\n",
    "6. Convert 'state_name', 'county_name' to lat and lon using the JSON file.\n",
    "7. Fix the schema for the columns which will become the index (lat, lon, year)\n",
    "8. Change precision of lat and lon\n",
    "9. Regroup by year, lat, lon and aggregate by sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NA columns\n",
    "combined = _load('aqi').dropna(axis=1)\n",
    "\n",
    "# Select relevant columns\n",
    "sel = ['State Name', 'county Name', 'Date', 'AQI', 'Defining Parameter']\n",
    "combined = combined[sel]\n",
    "\n",
    "# Clean column names\n",
    "combined.columns = [c.replace(\" \", \"_\").lower() for c in combined.columns]\n",
    "\n",
    "# Convert date column to year only\n",
    "combined.date = combined.date.str.split('-').str[0]\n",
    "combined = combined.rename(columns={'date': 'year'})\n",
    "combined = combined.astype({'year': 'int64'})\n",
    "\n",
    "# Pivot defining_parameter column\n",
    "d = pd.get_dummies(combined['defining_parameter'], prefix='defining')\n",
    "df = pd.concat([combined, d], axis=1).drop(['defining_parameter'], axis=1)\n",
    "df = df.dropna().groupby(by=['year', 'state_name', 'county_name'], as_index=False).mean()\n",
    "\n",
    "# Convert to Lat/Lon\n",
    "loc = list(df.state_name + \", \" + df.county_name)\n",
    "\n",
    "lat = [_get_lat_lon(i)[0] for i in loc]\n",
    "lon = [_get_lat_lon(i)[1] for i in loc]\n",
    "\n",
    "df['lat'] = _change_precision(lat)\n",
    "df['lon'] = _change_precision(lon)\n",
    "\n",
    "# Fix Schema\n",
    "df = df.dropna()\n",
    "df.year = df.year.astype(int)\n",
    "df.lat = df.lat.astype(int)\n",
    "df.lon = df.lon.astype(int)\n",
    "\n",
    "# Group up\n",
    "df = df.dropna().groupby(by=['year', 'lat', 'lon'], as_index=False).sum()\n",
    "\n",
    "aqi=df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LOAD CANCER DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the Cancer dataset into a common format, we will be performing following transformations:\n",
    "\n",
    "1. Clean the column names to remove spaces and capitals\n",
    "2. Select relevant columns (as defined in function)\n",
    "3. Pivot on leading_cancer_sites since it is categorical.\n",
    "4. Fix the MSA column \n",
    "5. Convert the cleaned MSA column to lat and lon.\n",
    "6. Fix the schema for the columns which will become the index (lat, lon, year)\n",
    "7. Change precision of lat and lon\n",
    "8. Regroup by year, lat, lon and aggregate by sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('data/raw/', 'health/United States and Puerto Rico Cancer Statistics, 1999-2016 Incidence.txt')\n",
    "cancer = pd.read_csv(path, delimiter='\\t')\n",
    "\n",
    "# Select relevant columns\n",
    "sel = ['Year', 'Leading Cancer Sites', 'MSA', 'Count', 'Population', 'Age-Adjusted Rate']\n",
    "cancer = cancer[sel]\n",
    "cancer.columns = [c.replace(' ', '_').lower() for c in cancer.columns]\n",
    "\n",
    "# Pivot criteria\n",
    "d = pd.get_dummies(cancer['leading_cancer_sites'], prefix='cancer')\n",
    "d = d.multiply(cancer['count'], axis=\"index\")\n",
    "cancer = pd.concat([cancer, d], axis=1).drop(['leading_cancer_sites', 'count'], axis=1)\n",
    "\n",
    "# Fix MSA\n",
    "tmp = cancer.msa\n",
    "ntmp = []\n",
    "\n",
    "# Also load manual map\n",
    "manual = {}\n",
    "with open('data/cancer_manual_map.txt', 'r') as fp:\n",
    "    for l in fp:\n",
    "        items = l.split('|')\n",
    "        manual[items[0].strip()] = items[1].strip()\n",
    "\n",
    "# MSA Conversion Policy\n",
    "for t in tmp:\n",
    "    t = str(t)\n",
    "    t = t.replace('-', ' ')\n",
    "    if len(re.split(',|,,', t)) == 1:\n",
    "        ntmp.append(t)\n",
    "        continue\n",
    "\n",
    "    states = re.split(',|,,', t)[1]\n",
    "    state = states.split()[0]\n",
    "\n",
    "    areas = t.split()\n",
    "    if len(areas) > 1:\n",
    "        area = t.split()[0] + ' ' + t.split()[1]\n",
    "    else:\n",
    "            area = areas[0]\n",
    "\n",
    "    tt = area + \", \" + state\n",
    "\n",
    "    if tt in manual:\n",
    "        ntmp.append(manual[tt])\n",
    "    else:\n",
    "        ntmp.append(tt)\n",
    "\n",
    "cancer.msa = ntmp\n",
    "cancer = cancer.dropna()\n",
    "\n",
    "    # Convert to Lat/Lon\n",
    "loc = list(cancer.msa)\n",
    "\n",
    "lat = [_get_lat_lon(i)[0] for i in loc]\n",
    "lon = [_get_lat_lon(i)[1] for i in loc]\n",
    "\n",
    "cancer['lat'] = _change_precision(lat, prec=0)\n",
    "cancer['lon'] = _change_precision(lon, prec=0)\n",
    "\n",
    "    # Fix Schema\n",
    "cancer = cancer.dropna()\n",
    "cancer.year = cancer.year.astype(int)\n",
    "cancer.lat = cancer.lat.astype(int)\n",
    "cancer.lon = cancer.lon.astype(int)\n",
    "\n",
    "    # Group up\n",
    "cancer = cancer.dropna().groupby(by=['year', 'lat', 'lon'], as_index=False).sum()\n",
    "\n",
    "cancer = cancer.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. LOAD LIFE EXPECTANCY DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the life expectancy dataset into a common format, we will be performing following transformations:\n",
    "\n",
    "1. Clean the column names to remove spaces and capitals\n",
    "2. Select relevant columns \n",
    "3. Fix the Life Exp range to Min and Max values\n",
    "4. Group by state and county, aggregate by mean to get the average life exp for that state and county\n",
    "5. Convert state and county to lat and lon using the JSON file\n",
    "6. Fix the schema for the columns which will become the index (lat, lon, year)\n",
    "7. Change precision of lat and lon\n",
    "8. Regroup by year, lat, lon and aggregate by sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('data/raw/', 'health', 'U.S._Life_Expectancy_at_Birth_by_State_and_Census_Tract_-_2010-2015.csv')\n",
    "life = pd.read_csv(path)\n",
    "\n",
    "    # Select Relevant\n",
    "sel = ['State', 'County', 'Life Expectancy',\n",
    "           'Life Expectancy Range']\n",
    "life = life[sel].dropna()\n",
    "\n",
    "    # Fix column names\n",
    "life.columns = [c.replace(' ', '_').lower() for c in life.columns]\n",
    "\n",
    "    # Fix Life Exp Range to Min and Max\n",
    "rng = list(life.life_expectancy_range)\n",
    "rng_max = [float(str(r).split('-')[1].strip()) for r in rng]\n",
    "rng_min = [float(str(r).split('-')[0].strip()) for r in rng]\n",
    "\n",
    "life['life_expectancy_max'] = rng_max\n",
    "life['life_expectancy_min'] = rng_min\n",
    "life = life.rename(columns={'life_expectancy': 'life_expectancy_avg'})\n",
    "life = life.drop('life_expectancy_range', axis=1).dropna()\n",
    "\n",
    "life = life.groupby(['state', 'county'], as_index=False).mean()\n",
    "\n",
    "    # Convert to Lat/Lon\n",
    "print(\"Getting lat lon list\")\n",
    "loc = list(life.state + \", \" + life.county)\n",
    "\n",
    "lat = [_get_lat_lon(i)[0] for i in loc]\n",
    "lon = [_get_lat_lon(i)[1] for i in loc]\n",
    "\n",
    "life['lat'] = _change_precision(lat, prec=0)\n",
    "life['lon'] = _change_precision(lon, prec=0)\n",
    "\n",
    "    # Fix Schema\n",
    "life = life.dropna()\n",
    "life.lat = life.lat.astype(int)\n",
    "life.lon = life.lon.astype(int)\n",
    "\n",
    "    # Group up\n",
    "life = life.dropna().groupby(by=['lat', 'lon'], as_index=False).sum()\n",
    "\n",
    "life = life.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. MERGE ALL DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now merge the datasets by year, lat, and lon. This is then written to a file that will become our main dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[ LOAD ] Loading AQI...\")\n",
    "print(aqi)\n",
    "\n",
    "print(\"[ LOAD ] Loading TRI...\")\n",
    "print(tri)\n",
    "\n",
    "print(\"[ LOAD ] Loading Life...\")\n",
    "print(life)\n",
    "\n",
    "print(\"[ LOAD ] Loading Cancer...\")\n",
    "print(cancer)\n",
    "\n",
    "# Join\n",
    "j = aqi.merge(tri, on=['year', 'lat', 'lon'], how='inner')\n",
    "j = j.merge(cancer, on=['year', 'lat', 'lon'], how='inner')\n",
    "j = j.merge(life, on=['lat', 'lon'], how='left')\n",
    "j = j.dropna()\n",
    "\n",
    "# TODO: Fix weird behavior with duplicated rows in merge that make this line necessary\n",
    "j = j.groupby(by=['year', 'lat', 'lon'], as_index=False).mean()\n",
    "\n",
    "j.to_csv('data/mergeddata.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
